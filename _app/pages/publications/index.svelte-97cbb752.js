import{S as N,i as R,s as C,e as v,j as m,c as y,a as b,m as p,d as u,b as _,f as h,o as g,x as d,u as f,v as $,k as w,t as T,n as A,g as q,H as I,C as P,r as L,w as M,R as V}from"../../chunks/index-fb3eacf8.js";import{C as W}from"../../chunks/Col-213ece60.js";import{C as G}from"../../chunks/Container-ac0d5b81.js";import{R as j}from"../../chunks/Row-1273e342.js";import{b as D}from"../../chunks/paths-28a87002.js";import{N as F}from"../../chunks/Nav-4e798ce2.js";import H from"./pdf.svelte-abee3957.js";import"../../chunks/preload-helper-e4860ae8.js";const z=[{name:"AQUATk: An Audio Assessment Toolkit",conf:"ISMIR 2023, Late Breaking Demo",authors:"Ashvala Vinay, Alexander Lerch",year:2023,url:"ismir23.pdf",abstract:"Recent advancements in Neural Audio Synthesis (NAS) have outpaced the development of standardized evaluation methodologies and tools. To bridge this gap, we introduce AquaTk, an open-source Python library specifically designed to simplify and standardize the evaluation of NAS systems. AquaTk offers a range of audio quality metrics, including a unique Python implementation of the basic PEAQ algorithm, and operates in multiple modes to accommodate various user needs."},{name:"Evaluating Generative Audio Systems and Their Metrics",conf:"ISMIR 2022",authors:"Ashvala Vinay, Alexander Lerch",year:2022,url:"ismir22.pdf",abstract:"Recent years have seen considerable advances in audio synthesis with deep generative models. However, the state-of-the-art is very difficult to quantify; different studies often use different evaluation methodologies and different metrics when reporting results, making a direct comparison to other systems difficult if not impossible. Furthermore, the perceptual relevance and meaning of the reported metrics in most cases unknown, prohibiting any conclusive insights with respect to practical usability and audio quality. This paper presents a study that investigates state-of-the-art approaches side-by-side with (i) a set of previously proposed objective metrics for audio reconstruction, and with (ii) a listening study. The results indicate that currently used objective metrics are insufficient to describe the perceptual quality of current systems."},{name:"Mind the Beat: Detecting Audio Onsets from EEG Recordings of Music Listening",conf:"ICASSP 2021",authors:"Ashvala Vinay, Alexander Lerch, Grace Leslie",year:2021,url:"icassp21.pdf",abstract:"We propose a deep learning approach to predicting audio event onsets in electroencephalogram (EEG) recorded from users as they listen to music. We use a publicly available dataset containing ten contemporary songs and concurrently recorded EEG. We generate a sequence of onset labels for the songs in our dataset and trained neural networks (a fully connected network (FCN) and a recurrent neural network (RNN)) to parse one second windows of input EEG to predict one second windows of onsets in the audio. We compare our RNN network to both the standard spectral-flux based novelty function and the FCN. We find that our RNN was able to produce results that reflected its ability to generalize better than the other methods.Since there are no pre-existing works on this topic, the numbers presented in this paper may serve as useful benchmarks for future approaches to this research problem."},{name:"The Impact of Salient Musical Features in a Hybrid Recommendation System for a Sound Library",conf:"Joint Proceedings of the ACM IUI Workshops, 2023",authors:"Jason Smith, Ashvala Vinay, Jason Freeman",year:2023,url:"iui23.pdf",abstract:"EarSketch is an online learning environment that teaches coding and music concepts through the computational manipulation of sounds selected from a large sound library. It features sound recommendations based on acoustic similarity and co-usage with a user's current sound selection in order to encourage exploration of the library. However, students have reported that the recommended sounds do not complement their current projects in terms of two areas: musical key and rhythm. We aim to improve the relevance of these recommendations through the inclusion of these two musically related features. This paper describes the addition of key signature and beat extraction to the EarSketch sound recommendation model in order to improve the musical compatibility of the recommendations with the sounds in a user\u2019s project. Additionally, we present an analysis of the effects of these new recommendation strategies on user exploration and usage of the recommended sounds. The results of this analysis suggest that the addition of explicitly musically-relevant attributes increases the coverage of the sound library among sound recommendations as well as the sounds selected by users. It reflects the importance of including multiple musical attributes when building recommendation systems for creative and open-ended musical systems."}];var x={publications:z};function S(l,s,o){const a=l.slice();return a[1]=s[o],a}function E(l){let s,o,a,e;return o=new H({props:{url:D+"/pdfs/"+l[1].url,meta:l[1]}}),{c(){s=v("div"),m(o.$$.fragment),a=w(),this.h()},l(r){s=y(r,"DIV",{class:!0});var n=b(s);p(o.$$.fragment,n),a=A(n),n.forEach(u),this.h()},h(){_(s,"class","pdf_container svelte-tf26pt")},m(r,n){h(r,s,n),g(o,s,null),I(s,a),e=!0},p:P,i(r){e||(d(o.$$.fragment,r),e=!0)},o(r){f(o.$$.fragment,r),e=!1},d(r){r&&u(s),$(o)}}}function B(l){let s,o,a=x.publications,e=[];for(let n=0;n<a.length;n+=1)e[n]=E(S(l,a,n));const r=n=>f(e[n],1,1,()=>{e[n]=null});return{c(){s=v("div");for(let n=0;n<e.length;n+=1)e[n].c();this.h()},l(n){s=y(n,"DIV",{class:!0});var i=b(s);for(let t=0;t<e.length;t+=1)e[t].l(i);i.forEach(u),this.h()},h(){_(s,"class","content")},m(n,i){h(n,s,i);for(let t=0;t<e.length;t+=1)e[t].m(s,null);o=!0},p(n,i){if(i&0){a=x.publications;let t;for(t=0;t<a.length;t+=1){const c=S(n,a,t);e[t]?(e[t].p(c,i),d(e[t],1)):(e[t]=E(c),e[t].c(),d(e[t],1),e[t].m(s,null))}for(L(),t=a.length;t<e.length;t+=1)r(t);M()}},i(n){if(!o){for(let i=0;i<a.length;i+=1)d(e[i]);o=!0}},o(n){e=e.filter(Boolean);for(let i=0;i<e.length;i+=1)f(e[i]);o=!1},d(n){n&&u(s),V(e,n)}}}function J(l){let s,o;return s=new W({props:{$$slots:{default:[B]},$$scope:{ctx:l}}}),{c(){m(s.$$.fragment)},l(a){p(s.$$.fragment,a)},m(a,e){g(s,a,e),o=!0},p(a,e){const r={};e&16&&(r.$$scope={dirty:e,ctx:a}),s.$set(r)},i(a){o||(d(s.$$.fragment,a),o=!0)},o(a){f(s.$$.fragment,a),o=!1},d(a){$(s,a)}}}function Q(l){let s,o,a,e,r,n,i;return s=new F({}),n=new j({props:{$$slots:{default:[J]},$$scope:{ctx:l}}}),{c(){m(s.$$.fragment),o=w(),a=v("h2"),e=T("Publications"),r=w(),m(n.$$.fragment),this.h()},l(t){p(s.$$.fragment,t),o=A(t),a=y(t,"H2",{class:!0});var c=b(a);e=q(c,"Publications"),c.forEach(u),r=A(t),p(n.$$.fragment,t),this.h()},h(){_(a,"class","svelte-tf26pt")},m(t,c){g(s,t,c),h(t,o,c),h(t,a,c),I(a,e),h(t,r,c),g(n,t,c),i=!0},p(t,c){const k={};c&16&&(k.$$scope={dirty:c,ctx:t}),n.$set(k)},i(t){i||(d(s.$$.fragment,t),d(n.$$.fragment,t),i=!0)},o(t){f(s.$$.fragment,t),f(n.$$.fragment,t),i=!1},d(t){$(s,t),t&&u(o),t&&u(a),t&&u(r),$(n,t)}}}function U(l){let s,o,a;return o=new G({props:{$$slots:{default:[Q]},$$scope:{ctx:l}}}),{c(){s=v("div"),m(o.$$.fragment),this.h()},l(e){s=y(e,"DIV",{class:!0});var r=b(s);p(o.$$.fragment,r),r.forEach(u),this.h()},h(){_(s,"class","mainContainer svelte-tf26pt")},m(e,r){h(e,s,r),g(o,s,null),a=!0},p(e,[r]){const n={};r&16&&(n.$$scope={dirty:r,ctx:e}),o.$set(n)},i(e){a||(d(o.$$.fragment,e),a=!0)},o(e){f(o.$$.fragment,e),a=!1},d(e){e&&u(s),$(o)}}}function O(l,s,o){return[!1]}class ne extends N{constructor(s){super(),R(this,s,O,U,C,{ssr:0})}get ssr(){return this.$$.ctx[0]}}export{ne as default};
